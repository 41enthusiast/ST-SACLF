wandb: Currently logged in as: mridulav (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /home2/txlx81/new_repos/temp_test/kaokore-visapp/wandb/run-20221026_183158-1wotult4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-pyramid-82
wandb: â­ï¸ View project at https://wandb.ai/mridulav/stcluster-classifier
wandb: ğŸš€ View run at https://wandb.ai/mridulav/stcluster-classifier/runs/1wotult4
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1823: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(

Global seed set to 42

  | Name  | Type    | Params
----------------------------------
0 | model | AttnVGG | 15.9 M
----------------------------------
1.2 M     Trainable params
14.7 M    Non-trainable params
15.9 M    Total params
63.465    Total estimated model params size (MB)
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)job exception: dictionary update sequence element #0 has length 6; 2 is required

wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.010 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: / 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: \ 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb: | 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ˆ
wandb:             lr-Adam â–â–
wandb:     train_acc_epoch â–ˆâ–
wandb:      train_acc_step â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–ˆâ–â–â–â–
wandb:    train_loss_epoch â–â–ˆ
wandb:     train_loss_step â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             val_acc â–â–ˆâ–ˆ
wandb:        val_accuracy â–â–ˆâ–ˆ
wandb:              val_f1 â–â–ˆâ–ˆ
wandb:            val_loss â–ˆâ–â–
wandb:       val_precision â–â–ˆâ–ˆ
wandb:          val_recall â–â–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 2
wandb:             lr-Adam 0.001
wandb:     train_acc_epoch 0.18273
wandb:      train_acc_step 0.0
wandb:    train_loss_epoch 34.08215
wandb:     train_loss_step 41.68056
wandb: trainer/global_step 3008
wandb:             val_acc 0.08402
wandb:        val_accuracy 0.08373
wandb:              val_f1 0.07987
wandb:            val_loss 38.21036
wandb:       val_precision 0.0796
wandb:          val_recall 0.08019
wandb: 
wandb: Synced giddy-pyramid-82: https://wandb.ai/mridulav/stcluster-classifier/runs/1wotult4
wandb: Synced 5 W&B file(s), 4 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221026_183158-1wotult4/logs
Traceback (most recent call last):
  File "train.py", line 308, in <module>
    best = fmin(fn=main,
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 586, in fmin
    rval.exhaust()
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 364, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 300, in run
    self.serial_evaluate()
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 178, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/base.py", line 897, in evaluate
    dict_rval = dict(rval)
ValueError: dictionary update sequence element #0 has length 6; 2 is required
Traceback (most recent call last):
  File "train.py", line 308, in <module>
    best = fmin(fn=main,
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 586, in fmin
    rval.exhaust()
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 364, in exhaust
    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 300, in run
    self.serial_evaluate()
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/fmin.py", line 178, in serial_evaluate
    result = self.domain.evaluate(spec, ctrl)
  File "/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/hyperopt/base.py", line 897, in evaluate
    dict_rval = dict(rval)
ValueError: dictionary update sequence element #0 has length 6; 2 is required
